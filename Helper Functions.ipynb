{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get learning rate \n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to count the number of correct predictions per data batch:\n",
    "def metrics_batch(output, target):\n",
    "    # get output class\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    \n",
    "    # compare output class with target class\n",
    "    corrects=pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to compute the loss value per batch of data:\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    \n",
    "    # get loss \n",
    "    loss = loss_func(output, target)\n",
    "    \n",
    "    # get performance metric\n",
    "    metric_b = metrics_batch(output,target)\n",
    "    \n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to compute the loss value and the performance metric for the entire dataset or an epoch.\n",
    "def loss_epoch(device,model,loss_func,dataset_dl,sanity_check=False,opt=None):\n",
    "    device = device\n",
    "    running_loss=0.0\n",
    "    running_metric=0.0\n",
    "    len_data=len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        # move batch to device\n",
    "        xb=xb.to(device)\n",
    "        yb=yb.to(device)\n",
    "        \n",
    "        # get model output\n",
    "        output=model(xb)\n",
    "        \n",
    "        # get loss per batch\n",
    "        loss_b,metric_b=loss_batch(loss_func, output, yb, opt) # changed output to output[0]\n",
    "        \n",
    "        # update running loss\n",
    "        running_loss+=loss_b\n",
    "        \n",
    "        # update running metric\n",
    "        if metric_b is not None:\n",
    "            running_metric+=metric_b\n",
    "\n",
    "        # break the loop in case of sanity check\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "    \n",
    "    # average loss value\n",
    "    loss=running_loss/float(len_data)\n",
    "    \n",
    "    # average metric value\n",
    "    metric=running_metric/float(len_data)\n",
    "    \n",
    "    return loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_val(device,model, params):\n",
    "    # extract model parameters\n",
    "    device=device\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "    \n",
    "    # history of loss values in each epoch\n",
    "    loss_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    # histroy of metric values in each epoch\n",
    "    metric_history={\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "    }\n",
    "    \n",
    "    # a deep copy of weights for the best performing model\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # initialize best loss to a large value\n",
    "    best_loss=float('inf')\n",
    "    \n",
    "    # main loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time()\n",
    "        # get current learning rate\n",
    "        current_lr=get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
    "        \n",
    "        # train model on training dataset\n",
    "        model.train()\n",
    "        train_loss, train_metric=loss_epoch(device,model,loss_func,train_dl,sanity_check,opt)\n",
    "\n",
    "        # collect loss and metric for training dataset\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        metric_history[\"train\"].append(train_metric)\n",
    "        \n",
    "        # evaluate model on validation dataset    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric=loss_epoch(device,model,loss_func,val_dl,sanity_check)\n",
    "        epoch_end_time = time()\n",
    "        epoch_total_time = epoch_end_time - epoch_start_time\n",
    "       \n",
    "        # store best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # store weights into a local file\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print(\"Copied best model weights!\")\n",
    "        \n",
    "        # collect loss and metric for validation dataset\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        metric_history[\"val\"].append(val_metric)\n",
    "        \n",
    "        # learning rate schedule\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(\"train loss: %.6f, valid loss: %.6f, train accuracy: %.2f, valid accuracy: %.2f, time: %.2f\" %(train_loss,val_loss,100*train_metric,100*val_metric,epoch_total_time))\n",
    "        print(\"-\"*10) \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing/predicting\n",
    "def deploy_model(model,dataset,device, num_classes=10,sanity_check=False):\n",
    "\n",
    "    len_data=len(dataset)\n",
    "    \n",
    "    # initialize output tensor on CPU: due to GPU memory limits\n",
    "    y_out=torch.zeros(len_data,num_classes)\n",
    "    \n",
    "    # initialize ground truth on CPU: due to GPU memory limits\n",
    "    y_gt=np.zeros((len_data),dtype=\"uint8\")\n",
    "    \n",
    "    # move model to device\n",
    "    model=model.to(device)\n",
    "    \n",
    "    elapsed_times=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(len_data):\n",
    "            x,y=dataset[i]\n",
    "            y_gt[i]=y\n",
    "            start=time()    \n",
    "            yy=model(x.unsqueeze(0).to(device))\n",
    "            y_out[i]=torch.softmax(yy,dim=1)\n",
    "            elapsed=time()-start\n",
    "            elapsed_times.append(elapsed)\n",
    "\n",
    "            if sanity_check is True:\n",
    "                break\n",
    "\n",
    "    inference_time=np.mean(elapsed_times)*1000\n",
    "    print(\"average inference time per image on %s: %.2f ms \" %(device,inference_time))\n",
    "    return y_out.numpy(),y_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to show images\n",
    "def imgshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buan6341_2020",
   "language": "python",
   "name": "buan6341_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
